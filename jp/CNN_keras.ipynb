{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畳み込みニューラルネットワーク（CNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みとは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src = \"../fig/cnn1.png\" width=250 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32 $*$ 32ピクセル（長さと幅）3スペクトルバンド（3チャンネル）、5 $*$ 5 kernel_sizeサイズ[1]の画像の例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下の図は、それがどのように正確に機能するかを示しています"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../fig/cnn2.png\" width = 350 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各フィルターは、独立して画像と畳み込まれ、ユーザーが定義した数の機能になります。 次の図では、6つのフィルターから派生した6つのフィーチャ表現があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../fig/cnn3.png\" width = 350 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、CNNは定義された数の畳み込み層（ブロック）のコレクションであり、1つの層からの畳み込み（機能）結果は、次の図に示すように入力として次の層に渡されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../fig/cnn4.png\" width=300 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kerasを使用して独自のシンプルなCNNを作成して、詳細を見てみましょう**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "x_train,y_train = np.load('../dataset/isprs_vaihingen/train/patches/image.npy'),np.load('../dataset/isprs_vaihingen/train/patches/label.npy')\n",
    "x_test,y_test = np.load('../dataset/isprs_vaihingen/val/patches/image.npy'),np.load('../dataset/isprs_vaihingen/val/patches/label.npy')\n",
    "channel = x_train.shape[-1]\n",
    "num_classes = 5\n",
    "img_rows, img_cols = 256, 256\n",
    "input_shape = (img_rows, img_cols, channel)\n",
    "y_tra = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_tes = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,Input,Activation,Conv2D, MaxPooling2D,UpSampling2D, Dropout,Conv2DTranspose, concatenate, Flatten,BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**フィルター**、**カーネルサイズ**、**パディング**、**ストライド**、**プーリング**および生成された**パラメーター**の数を確認してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 10, kernel_size = (5,5), strides =1,padding = 'valid', \n",
    "                 input_shape=input_shape, activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**パディングとは**  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**パディング**は、畳み込み後の画像の元の形状を保持するために、画像の外側に余分なピクセルを追加することです[2]。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/no_padding_no_strides.gif\" width = 250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/arbitrary_padding_no_strides.gif\" width = 250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 10, kernel_size = (5,5), strides = 1, padding = 'same', \n",
    "                 input_shape=input_shape, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ストライド**は、カーネルの畳み込みに使用されるピクセルの単位です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/no_padding_strides.gif\" width = 250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../fig/padding_strides.gif\" width = 250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 10, kernel_size = (5,5), strides = 2, padding = 'same', \n",
    "                 input_shape=input_shape, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**プールレイヤー**は、各入力フィーチャで独立してボリュームを空間的にダウンサンプリングします。 この関数は、空間サイズを縮小するために使用し、したがってパラメーターと計算の数を減らします。 例：maxpool、averagepool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../fig/pooling.png\" width = 350 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 10, kernel_size = (5,5), strides = 1, padding = 'same', \n",
    "                 input_shape=input_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kerasで Encoder-decoder CNNネットワークを構築**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはCNNネットワークで、エンコーダーネットワーク、対応するデコーダーネットワーク、それに続くピクセル単位の分類レイヤーで構成されます。Segnet、U-net、Deconv-netなど、いくつかの有名なエンコーダーデコーダーが利用可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "独自のシンプルなEncoder-decoderネットワークを定義しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、エンコーダー部分を定義します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((input_shape))\n",
    "conv1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.int_shape(pool2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デコーダー部分を定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = Conv2D(32, (3, 3), activation='relu', padding='same')(pool2)\n",
    "up1 = UpSampling2D(size=(2, 2),interpolation='nearest')(conv3)\n",
    "conv4 = Conv2D(16, (3, 3), activation='relu', padding='same')(up1)\n",
    "up2 = UpSampling2D(size=(2, 2),interpolation='nearest')(conv4)\n",
    "conv5 = Conv2D(num_classes, (1, 1), activation='softmax')(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.int_shape(conv5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[inputs], outputs=[conv5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 32\n",
    "EPOCHS = 2\n",
    "#SGD = keras.optimizers.SGD(lr=0.01)\n",
    "SGD = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "catergorical = keras.losses.categorical_crossentropy\n",
    "accuracy = ['accuracy']\n",
    "model.compile(loss = catergorical, optimizer = SGD,\n",
    "              metrics = accuracy)\n",
    "\n",
    "history1 = model.fit(x_train, y_tra,\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data = (x_test, y_tes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測のためのデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred,y_pred = np.load('../dataset/isprs_vaihingen/test/patches/image.npy'),np.load('../dataset/isprs_vaihingen/test/patches/label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_prob = model.predict(x_pred)\n",
    "Predict_prob.shape\n",
    "Predict_class = np.argmax(Predict_prob,axis=-1)\n",
    "Predict_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib  notebook\n",
    "import matplotlib.pyplot as  plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(Predict_class[4], interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Predict')\n",
    "fig.show()\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(y_pred[4], interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Label')\n",
    "\n",
    "fig.suptitle('Scene: top_mosaic_09cm_area1')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**U-net; 深い畳み込みネットワーク**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-netは、U字型のエンドツーエンドネットワークです。 U-netは、縮小パス（左側）と拡張パス（右側）で構成されています。 U-netには、ダウンサンプリングプロセスで失われた機能をアップサンプリングプロセスで適切な機能と連結する特別なアーキテクチャがあります。 この特定の側面により、U-netはエレガントな深層学習ネットワークとして機能します[3]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../fig/unet.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((input_shape))\n",
    "# Encoder\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "# Decoder\n",
    "conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "conv6 = BatchNormalization()(conv6)\n",
    "conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "conv7 = BatchNormalization()(conv7)\n",
    "conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "conv8 = BatchNormalization()(conv8)\n",
    "conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "conv9 = BatchNormalization()(conv9)\n",
    "conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "conv10 = Conv2D(num_classes, (1, 1), activation='softmax')(conv9)\n",
    "model = Model(inputs=[inputs], outputs=[conv10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 24\n",
    "EPOCHS = 1\n",
    "#SGD = keras.optimizers.SGD(lr=0.01)\n",
    "SGD = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "catergorical = keras.losses.categorical_crossentropy\n",
    "accuracy = ['accuracy']\n",
    "model.compile(loss = catergorical, optimizer = SGD,\n",
    "              metrics = accuracy)\n",
    "\n",
    "history1 = model.fit(x_train, y_tra,\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data = (x_test, y_tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"../learned_weights/demo_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../learned_weights/demo_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_prob = model.predict(x_pred)\n",
    "Predict_prob.shape\n",
    "Predict_class = np.argmax(Predict_prob,axis=-1)\n",
    "Predict_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(Predict_class[1], interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Predict')\n",
    "fig.show()\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(y_pred[1], interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Label')\n",
    "\n",
    "fig.suptitle('Scene: top_mosaic_09cm_area1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py\n",
    "json_file = open('../learned_weights/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../learned_weights/model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_prob = loaded_model.predict(x_pred)\n",
    "Predict_prob.shape\n",
    "Predict_class = np.argmax(Predict_prob,axis=-1)\n",
    "Predict_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(Predict_class[1], interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Predict')\n",
    "fig.show()\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(y_pred[1], interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Label')\n",
    "\n",
    "fig.suptitle('Scene: top_mosaic_09cm_area1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参照**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Harsh Pokharna, The best explanation of Convolutional Neural Networks on the Internet\n",
    "2. Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning (BibTeX)\n",
    "3. Olaf Ronneberger, Philipp Fischer, Thomas Brox, U-Net: Convolutional Networks for Biomedical Image Segmentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
