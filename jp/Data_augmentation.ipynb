{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容\n",
    "## PyTorchの基本\n",
    "- ネットワークアーキテクチャの定義\n",
    "- データセットとデータローダー\n",
    "- トレーニング\n",
    "- 推論\n",
    "\n",
    "## データ増強\n",
    "- データ増強"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マサチューセッツ州の建物のデータセット\n",
    "このチュートリアルでは、建物の検出にデータセットを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fpath_image = '../dataset/building_vmnih/test/image/22828930_15.tiff'\n",
    "fpath_label = '../dataset/building_vmnih/test/label/22828930_15.tif'\n",
    "\n",
    "image = np.array(Image.open(fpath_image))\n",
    "label = np.array(Image.open(fpath_label))\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(image, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.show()\n",
    "ax.set_title('Source image')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(label, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Ground truth')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットでは、137組の画像と上記のようなラベルがトレーニング用に提供されています。データセットでモデルをトレーニングする前に、pytorchの基本について説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PyTorchの基本\n",
    "PyTorchは、Facebookが開発した有名なディープラーニングフレームワークです。ここでは、PyTorchの基本を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、pytorchモジュールをインポートします。 <br>\n",
    "`torch.nn`には、畳み込み、プーリング、バッチ正規化など、さまざまな種類の操作のクラスが含まれます。<br>\n",
    "`torch.nn.functional`には、アクティベーション関数などの基本的な（およびノンパラメトリックな）操作のための関数が含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchのテンソル\n",
    "テンソルはさまざまな方法で作成できます。 テンソルは、numpy配列のように使用できます。\n",
    "```\n",
    "torch.FloatTensor([1,2,3])\n",
    "torch.from_numpy(ndarray)\n",
    "torch.Tensor([1,2,3]).float()\n",
    "```\n",
    "\n",
    "### 基本操作\n",
    "ネットワークを実装する前に、畳み込みやプーリングなどの基本操作を定義する方法を簡単に確認します。 <br><br>\n",
    "\n",
    "**畳み込み層**<br>\n",
    "畳み込み層はクラス（ `nn.Conv2d`）として実装されます。 最初にレイヤーのインスタンスを作成し、それを関数のように使用します。\n",
    "```\n",
    "conv1 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "output = conv1(input)\n",
    "```\n",
    "\n",
    "**プール層**\n",
    "```\n",
    "pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "output = pool1(input)\n",
    "\n",
    "pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "output = pool2(input)\n",
    "```\n",
    "\n",
    "**完全に接続されたレイヤー**\n",
    "```\n",
    "fc1 = torch.nn.Linear(in_features=128, out_features=128)\n",
    "output = fc1(input)\n",
    "```\n",
    "\n",
    "**アクティベーション機能**<br>\n",
    "アクティベーションは関数として実装されます。\n",
    "```\n",
    "output = torch.nn.functional.relu(input)\n",
    "```\n",
    "クラスバージョンも使用できます。\n",
    "```\n",
    "act1 = torch.nn.ReLU()\n",
    "output = act1(input)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワークアーキテクチャの定義\n",
    "上記の基本操作を使用して、入力として$64\\times64$パッチを持つVGGのようなモデルを実装し、サイズ$16\\times16$の中心領域の推定値を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGGS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGS, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc6 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc7 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=256)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.dropout(self.fc6(x))\n",
    "        x = F.dropout(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        x = x.view(batch_size, 16, 16)\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを作成し、アーキテクチャを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGS()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムテンソルを処理して、アーキテクチャが正しいかどうかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.randn(100,3,64,64).astype(np.float32)\n",
    "inputs = torch.from_numpy(inputs)\n",
    "output = model(inputs)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "ここでは、トレーニングのクロスエントロピー損失関数を定義します。 `log()`でのオーバーフローを避けるために、小さな値`eps`が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Criterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Criterion, self).__init__()\n",
    "        self.eps = 1.0e-7\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        prob = F.sigmoid(inputs)\n",
    "        loss = -targets.float()*(prob + self.eps).log() - (1-targets.float())*(1 - prob + self.eps).log()\n",
    "        loss = loss.mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットとデータローダー\n",
    "以下の図は、トレーニング用のミニバッチがPyTorchでどのように作成されるかを示しています。 <br>\n",
    "**DataLoader**: DataLoaderはDatasetからデータを取得し、収集したデータをミニバッチにパックします。 <br>\n",
    "**Dataset**: データローダから要求されたデータセットのロードと前処理データ。 場合によっては、独自のデータ用にカスタマイズされたデータセットを実装する必要があります。 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/pth_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独自のデータセットを定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Buildings(Dataset):\n",
    "    def __init__(self, fpath_image_npy, fpath_label_npy):\n",
    "        self.images = np.load(fpath_image_npy)\n",
    "        self.labels = np.load(fpath_label_npy)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        image = image.transpose([2,0,1])    # (H,W,B) to (B,H,W)\n",
    "        \n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        label_tensor = torch.from_numpy(label).long()\n",
    "        \n",
    "        return image_tensor, label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建物データセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Buildings('../dataset/building_vmnih/train/patches/sat.npy', '../dataset/building_vmnih/train/patches/map.npy')\n",
    "image_tensor, label_tensor = dataset[0]\n",
    "print(image_tensor.size())\n",
    "print(label_tensor.size())\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、DataLoaderを設定し、機能するかどうかを確認します。 <br><br>\n",
    "DataLoaderのinitの引数: <br>\n",
    "batch_size: batch_sizeを指定します<br>\n",
    "shuffle: Trueの場合、データはDatasetからランダムにサンプリングされ、Falseの場合、データは順次サンプリングされます。<br>\n",
    "num_workers: マルチスレッドワーカーの数。 データを並行してロードできるため、データのロードが高速化されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "for image, label in loader:\n",
    "    print(image.size(), torch.mean(image).item())\n",
    "    print(label.size(), label[0,0,0].item())\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、トレーニングデータローダーを準備しました。 以下では、トレーニング手順を簡単に示します。 完全なトレーニングには多くの時間がかかるため、最初のいくつかの反復を示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デバイスを設定します。 この場合、 \"cpu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングする最大エポックを設定します。 デモンストレーションのために、max_epochs 1を設定しますが、数十個のエポックを使用することをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルをデバイスに転送"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オプティマイザーを設定します。 ここでは、Adamを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.999), weight_decay=1.0e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数を設定し、デバイスに転送します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Criterion().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングの繰り返し。 いくつかの反復のみを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    for idx, (image, label) in enumerate(loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        \n",
    "        # At the begining of each iteraton, clear accumulated gradient for the network parameters.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Input the image to the model and get the prediction.\n",
    "        output = model(image)\n",
    "        \n",
    "        # Calculate the loss function comparing the prediction and the ground truth\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # Backpropagate the error signal through the model to calculate gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters using the calculated gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch #%d, batch #%d: loss=%f' % (epoch, idx, loss.item()))\n",
    "        \n",
    "        # For demonstration, stop iteration at iter5\n",
    "        if idx == 4:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習したモデルを保存する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力ディレクトリを設定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = '../learned_weights/'\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習した重みをdictionaryとして取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_weights = model.state_dict()\n",
    "for key, weight in dict_weights.items():\n",
    "    print('%s\\t%s' % (key, weight.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionaryを保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dict_weights, output_dir + '/demo_building_vggs.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定。 切り取りウィンドウを16ストライドずつスライドさせて、テスト画像を切り取ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "aoi_size = 16\n",
    "stride = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/fig_test_patch_small.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト領域の画像を準備する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Load image\n",
    "fpath_test_image = '../dataset/building_vmnih/test/image/22828930_15.tiff'\n",
    "image = np.array(Image.open(fpath_test_image))\n",
    "org_height, org_width, _ = image.shape\n",
    "\n",
    "# Normalize\n",
    "mean = np.mean(image, axis=(0,1), keepdims=True)\n",
    "std = np.std(image, axis=(0,1), keepdims=True)\n",
    "image = (image - mean) / std\n",
    "\n",
    "# To tensor\n",
    "image = image.transpose([2,0,1])    # Swap dimensions, (H,W,B) to (B,H,W)\n",
    "image = image[np.newaxis,:,:,:]    # Add batch dimension (N,B,H,W)\n",
    "image = torch.from_numpy(image).float()    # Convert to float tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワークは入力パッチの中央領域の推定結果を出力するため、画像のエッジ領域を推定するために、入力パッチの一部が有効な画像領域外にあります。 したがって、パディングを使用して元の画像を拡大する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/fig_test_padding_small.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reflection padding\n",
    "pad_size = int((64-16)/2)\n",
    "pad = nn.ReflectionPad2d(pad_size)\n",
    "image = pad(image)\n",
    "_, _, height, width = image.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを設定し、トレーニング済みの重みを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model\n",
    "model = VGGS().to(device)\n",
    "\n",
    "# Load learned weights\n",
    "learned_weights = torch.load('../learned_weights/demo_building_vggs.torch')\n",
    "model.load_state_dict(learned_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set place holder\n",
    "prob_map = np.zeros([height, width])\n",
    "count_map = np.zeros([height, width])\n",
    "\n",
    "# Crop patches in sliding manner, and apply the prediction model\n",
    "num_tiles_x = (width - patch_size) // stride + 2\n",
    "num_tiles_y = (height - patch_size) // stride + 2\n",
    "\n",
    "for iy in range(num_tiles_y):\n",
    "    for ix in range(num_tiles_x):\n",
    "        print('(%d/%d, %d/%d)' % (iy, num_tiles_y, ix, num_tiles_x))\n",
    "        ulx = ix * stride\n",
    "        uly = iy * stride\n",
    "        lrx = ulx + patch_size\n",
    "        lry = uly + patch_size\n",
    "        \n",
    "        if lrx > width:\n",
    "            ulx = width - patch_size\n",
    "            lrx = width\n",
    "            \n",
    "        if lry > height:\n",
    "            uly = height - patch_size\n",
    "            lry = height\n",
    "            \n",
    "        patch = image[:, :, uly:lry, ulx:lrx]\n",
    "        patch = patch.to(device)\n",
    "        \n",
    "        logit = model(patch)\n",
    "        prob = F.sigmoid(logit)\n",
    "        \n",
    "        stx = ulx + int((patch_size - aoi_size) / 2)\n",
    "        sty = uly + int((patch_size - aoi_size) / 2)\n",
    "        prob_map[sty:sty+aoi_size, stx:stx+aoi_size] += prob.detach().cpu().numpy().squeeze()\n",
    "        count_map[sty:sty+aoi_size, stx:stx+aoi_size] += np.ones([aoi_size,aoi_size])\n",
    "\n",
    "# Eliminate padded region\n",
    "prob_map = prob_map[pad_size:pad_size+org_height, pad_size:pad_size+org_width]\n",
    "count_map = count_map[pad_size:pad_size+org_height, pad_size:pad_size+org_width]\n",
    "\n",
    "# Take average for overlaped regions\n",
    "result = prob_map / count_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推定結果を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fpath_test_label = '../dataset/building_vmnih/test/label/22828930_15.tif'\n",
    "label = np.array(Image.open(fpath_test_label))[:,:,0]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(result, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.show()\n",
    "ax.set_title('Estimation')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(label, interpolation='none')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Ground truth')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred = (result > 0.5)\n",
    "label = (label > 0.5)\n",
    "\n",
    "TP = np.sum(pred*label)\n",
    "T = np.sum(label)\n",
    "P = np.sum(pred)\n",
    "IoU = float(TP) / (T+P-TP) * 100\n",
    "print('IoU: %.2f%%' % IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、Datasetクラスにデータ拡張（ランダムな回転と反転）を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class Buildings(Dataset):\n",
    "    def __init__(self, fpath_image_npy, fpath_label_npy, augmentation=False):\n",
    "        self.images = np.load(fpath_image_npy)\n",
    "        self.labels = np.load(fpath_label_npy)\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Augmentation\n",
    "        if self.augmentation:\n",
    "            # Randomly choose rotation angle and flipping direction\n",
    "            rotation = random.choice([0, 90, 180, 270])\n",
    "            flip = random.choice(['H', 'V', 'N'])\n",
    "            \n",
    "            # Apply transformation for image\n",
    "            image = self._rotate(image, rotation)\n",
    "            image = np.array(self._flip(image, flip))\n",
    "\n",
    "            # Also, apply the same transformation for label\n",
    "            label = self._rotate(label, rotation)\n",
    "            label = np.array(self._flip(label, flip))\n",
    "        \n",
    "        image = image.transpose([2,0,1])    # (H,W,B) to (B,H,W)\n",
    "        \n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        label_tensor = torch.from_numpy(label).long()\n",
    "        \n",
    "        return image_tensor, label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    # Rotation function\n",
    "    def _rotate(self, image, rotation):\n",
    "        if rotation == 0:\n",
    "            return image\n",
    "        elif rotation == 90:\n",
    "            image = image.swapaxes(0,1)\n",
    "            return np.flip(image, axis=0)\n",
    "        elif rotation == 180:\n",
    "            image = np.flip(image, axis=0)\n",
    "            return np.flip(image, axis=1)\n",
    "        elif rotation == 270:\n",
    "            image = image.swapaxes(0,1)\n",
    "            return np.flip(image, axis=1)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "    # Flip function\n",
    "    def _flip(self, image, direction):\n",
    "        if direction == 'H':\n",
    "            return np.flip(image, axis=1)\n",
    "        elif direction == 'V':\n",
    "            return np.flip(image, axis=0)\n",
    "        elif direction == 'N':\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットからいくつかのパッチをサンプリングして、拡張性を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib.gridspec import GridSpec\n",
    "grid = GridSpec(nrows=2, ncols=10)\n",
    "\n",
    "\n",
    "mean = np.array([75, 75, 75])\n",
    "std = np.array([50, 50, 50])\n",
    "\n",
    "mean = mean[:, np.newaxis, np.newaxis]\n",
    "std = std[:,np.newaxis, np.newaxis]\n",
    "\n",
    "dataset = Buildings('../dataset/building_vmnih/train/patches/sat.npy', '../dataset/building_vmnih/train/patches/map.npy', augmentation=True)\n",
    "\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "for i in range(10):\n",
    "    image_tensor, label_tensor = dataset[3]\n",
    "    image_patch = image_tensor.detach().numpy() * std + mean\n",
    "    label_patch = label_tensor.detach().numpy()\n",
    "    \n",
    "    image_patch = image_patch.transpose([1,2,0])\n",
    "    image_patch = image_patch.clip(0,255).astype(np.uint8)\n",
    "\n",
    "    ax = fig.add_subplot(grid[0,i])\n",
    "    ax.imshow(image_patch, interpolation='none')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax = fig.add_subplot(grid[1,i])\n",
    "    ax.imshow(label_patch, interpolation='none')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**より多くのエポックを持つ大規模データを使用して、訓練されたモデルの結果を表示する**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"../fig/bulding_detct.png\" width =600 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このような手法は、ラベル付きデータの量が限られている場合に特に効果的です。\n",
    "他の方法があります \n",
    "- 転移学習\n",
    "- データ融合\n",
    "- 拡張畳み込み\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
